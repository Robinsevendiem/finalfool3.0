import streamlit as st
import pandas as pd
import numpy as np
import os
import time
from autogluon.tabular import TabularPredictor
from sklearn.linear_model import LinearRegression
import altair as alt
import sys
import subprocess
import datetime
import os

# --- Config ---
st.set_page_config(
    page_title="èŠ±å§‘å¨˜2.0 AI æŠ•é¡¾",
    page_icon="ğŸŒ¸",
    layout="wide"
)

# --- Authentication Gate ---
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.write("# ğŸŒ¸ æ¬¢è¿è¿›å…¥ AI æŠ•é¡¾ç³»ç»Ÿ")
        st.write("---")
        st.write("### ğŸ” è®¿é—®éªŒè¯")
        answer = st.text_input("è¯·è¾“å…¥é€šå…³å£ä»¤ä»¥ç»§ç»­ï¼š", type="password", placeholder="è¯·è¾“å…¥ç­”æ¡ˆ...")
        if st.button("ç«‹å³è§£é”", use_container_width=True):
            if answer == "777":
                st.session_state.authenticated = True
                st.success("éªŒè¯é€šè¿‡ï¼æ­£åœ¨ä¸ºæ‚¨åŠ è½½ç³»ç»Ÿ...")
                time.sleep(1)
                st.rerun()
            else:
                st.error("å£ä»¤é”™è¯¯ï¼Œæ— æ³•è¿›å…¥ç³»ç»Ÿã€‚")
    st.stop()

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿æ¨¡å‹åŠ è½½ä¸å—è¿è¡Œç¯å¢ƒå½±å“
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Available Model Versions
MODEL_VERSIONS = {
    "æœ€å¼ºç‹è€…": os.path.join(BASE_DIR, "AutogluonModels/ag-20260122_050556"),
    "è¿›åŒ–å¤±è´¥": os.path.join(BASE_DIR, "AutogluonModels/ag-20260126_044254"),
    "ç»©æ•ˆä¼˜åŒ–ç‰ˆæœªæ¥10æ—¥": os.path.join(BASE_DIR, "AutogluonModels/performance_v1")
}

# Initialize session state for navigation and settings
if 'page' not in st.session_state:
    st.session_state.page = 'dashboard'
if 'selected_version' not in st.session_state or st.session_state.selected_version not in MODEL_VERSIONS:
    st.session_state.selected_version = list(MODEL_VERSIONS.keys())[0]

def navigate_to(page):
    st.session_state.page = page

MODEL_PATH = MODEL_VERSIONS[st.session_state.selected_version]
DATA_DIR = os.path.join(BASE_DIR, 'market_data')
WINDOWS = [3, 5, 10, 20, 23, 30, 60, 120]

NAME_MAP = {
    '513100.SH': 'çº³æŒ‡100',
    '513520.SH': 'æ—¥ç»ETF',
    '513500.SH': 'æ ‡æ™®500',
    '159915.SZ': 'åˆ›ä¸šæ¿',
    '588120.SH': 'ç§‘åˆ›æ¿',
    '588000.SH': 'ç§‘åˆ›æ¿', 
    '510180.SH': 'ä¸Šè¯180',
    '518880.SH': 'é»„é‡‘ETF',
    '511090.SH': '30å¹´å›½å€º',
    '161129.SZ': 'å—æ–¹åŸæ²¹',
    '501018.SH': 'å—æ–¹åŸæ²¹'
}
VALID_ASSETS = list(set(NAME_MAP.values()))

# --- Helper Functions ---

@st.cache_data(ttl=3600)  # Add TTL to auto-refresh cache
def load_market_data():
    data = {}
    if not os.path.exists(DATA_DIR): return {}
    for filename in os.listdir(DATA_DIR):
        if filename.endswith('.csv'):
            code = filename.split('_')[0]
            try:
                df = pd.read_csv(os.path.join(DATA_DIR, filename))
                df['date'] = pd.to_datetime(df['trade_date'].astype(str))
                df = df.sort_values('date').reset_index(drop=True)
                if 'close_qfq' in df.columns: df['close'] = df['close_qfq']
                if 'vol' in df.columns: df['volume'] = df['vol']
                
                name = None
                for k, v in NAME_MAP.items():
                    if k in filename or k == code:
                        name = v
                        break
                if name:
                    df['name'] = name
                    df['code'] = code
                    data[name] = df
            except Exception as e:
                print(f"Error loading {filename}: {e}")
    return data

@st.cache_resource(show_spinner=False)
def load_model(path=None):
    target_path = path if path else MODEL_PATH
    predictor_file = os.path.join(target_path, 'predictor.pkl')
    
    if not os.path.exists(predictor_file):
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¸è¦è®© cache_resource è®°ä½è¿™ä¸ª None ç»“æœ
        # æˆ‘ä»¬å¯ä»¥é€šè¿‡æŠ›å‡ºå¼‚å¸¸æˆ–è€…ä¸ä½¿ç”¨ç¼“å­˜çš„æ–¹å¼å¤„ç†
        return None
        
    try:
        return TabularPredictor.load(target_path)
    except Exception as e:
        print(f"Error loading model at {target_path}: {e}")
        return None

@st.cache_data
def calc_max_drawdown(prices):
    """Calculate Maximum Drawdown for a price series."""
    if len(prices) < 1: return 0.0
    # Calculate cumulative max
    roll_max = np.maximum.accumulate(prices)
    # Avoid division by zero
    if roll_max[0] == 0: return 0.0
    # Calculate drawdown
    drawdown = (prices - roll_max) / roll_max
    # Return max drawdown (min value, since dd is negative)
    return drawdown.min()

def prepare_all_features_cached(data_dict, windows, warmup=True, start_date=None):
    """
    Pre-calculate all static features.
    warmup: If True, uses full history. If False, masks data before start_date (simulating fresh start).
    """
    all_dates = set()
    for df in data_dict.values():
        all_dates.update(df['date'].tolist())
    sorted_dates = sorted(list(all_dates))
    
    # Process each asset
    processed_data = {} 
    
    for name, df in data_dict.items():
        sub = df.set_index('date').sort_index()
        df_feat = sub.copy()
        
        # If no warmup, we must mask data before start_date for calculation
        # But we still need the rows to exist.
        # Actually, if we don't warmup, the window functions at start_date will be NaN.
        # This is naturally handled if we just pass the full dataframe but the user accepts NaNs at the start.
        # However, if user explicitly wants "No History used", we should probably trim the input DF?
        # But rolling window NEEDS history. If you cut history, rolling window is NaN.
        # So "No Warmup" effectively means "First 30 days are NaN/Cash".
        
        # Let's keep calculation as is (vectorized on full data), 
        # but in the backtest loop, we can check if enough data is available *relative to start_date*?
        # No, simpler: just calculate. If data exists, it exists.
        # The user's request "Don't use history before selected date" implies:
        # On Day 0 of backtest, Ret_23 should be NaN (or based on 0 history).
        # This forces the model to see "Missing Data" and likely choose Cash.
        
        if not warmup and start_date:
            # Mask data before start_date
            # We can't delete rows because we need to iterate dates.
            # We can set values to NaN before start_date?
            # Better: Filter df to start from start_date
            df_feat = df_feat[df_feat.index >= pd.Timestamp(start_date)]
            
        # Pre-calc daily ret for backtest
        df_feat['daily_ret'] = df_feat['close'].pct_change().fillna(0.0)
            
        close_vals = df_feat['close'].values
        
        for w in windows:
            # Ret
            df_feat[f'ret_{w}'] = df_feat['close'].pct_change(w)
            
            # Vol
            df_feat[f'vol_{w}'] = df_feat['close'].pct_change().rolling(w).std() * np.sqrt(252)
            
            # Slope & R2 & MaxDD
            s_list = []
            r_list = []
            mdd_list = []
            # We need to re-index close_vals if we filtered
            curr_close = df_feat['close'].values
            
            for i in range(len(df_feat)):
                if i < w:
                    s_list.append(np.nan)
                    r_list.append(np.nan)
                    mdd_list.append(np.nan)
                else:
                    win = curr_close[i-w+1 : i+1]
                    s_list.append(calc_slope(win))
                    r_list.append(calc_r2(win))
                    mdd_list.append(calc_max_drawdown(win))
            df_feat[f'slope_{w}'] = s_list
            df_feat[f'r2_{w}'] = r_list
            df_feat[f'mdd_{w}'] = mdd_list
            df_feat[f'sxr_{w}'] = df_feat[f'slope_{w}'] * df_feat[f'r2_{w}']
            # æ–°å¢ï¼šé£é™©è°ƒæ•´ååŠ¨é‡
            df_feat[f'sharp_{w}'] = df_feat[f'slope_{w}'] / (df_feat[f'vol_{w}'] + 0.01)
             
        processed_data[name] = df_feat
        
    return processed_data, sorted_dates

def run_backtest_range(predictor, data_dict, start_date, end_date, model_name, initial_holding=None, force_neutral=False, use_warmup=True):
    # 1. Pre-calculate features
    with st.spinner("æ­£åœ¨é¢„è®¡ç®—å…¨é‡ç‰¹å¾..."):
        # We need to pass start_date if warmup is False
        s_str = str(start_date) if not use_warmup else None
        # Cache key must include warmup params
        processed_data, all_dates = prepare_all_features_cached(data_dict, WINDOWS, warmup=use_warmup, start_date=s_str)
    
    # Filter dates
    s_ts = pd.Timestamp(start_date)
    e_ts = pd.Timestamp(end_date)
    sim_dates = [d for d in all_dates if d >= s_ts and d <= e_ts]
    
    if not sim_dates:
        return None, "Selected range has no trading days."
        
    history = []
    current_holding = initial_holding
    
    progress_bar = st.progress(0)
    
    for i, d in enumerate(sim_dates):
        # Update progress
        progress_bar.progress((i + 1) / len(sim_dates))
        
        # Build features for this day
        daily_rows = []
        
        # Determine is_held status based on mode
        # If force_neutral is True, we always pretend we hold nothing (Opportunity Hunter Mode)
        effective_holding = None if force_neutral else current_holding
        
        # Real Assets
        for name, df in processed_data.items():
            if d in df.index:
                row = df.loc[d]
                if pd.notnull(row['slope_23']): # Valid
                    # Feature dict
                    feat = {
                        'name': name,
                        'is_held': 1 if effective_holding == name else 0
                    }
                    for w in WINDOWS:
                        feat[f'ret_{w}'] = row[f'ret_{w}']
                        feat[f'vol_{w}'] = row[f'vol_{w}']
                        feat[f'slope_{w}'] = row[f'slope_{w}']
                        feat[f'r2_{w}'] = row[f'r2_{w}']
                        feat[f'mdd_{w}'] = row[f'mdd_{w}']
                        feat[f'sxr_{w}'] = row[f'sxr_{w}']
                        feat[f'sharp_{w}'] = row[f'sharp_{w}']
                    daily_rows.append(feat)
        
        # Cash Asset
        cash_feat = {
            'name': 'ç°é‡‘',
            'is_held': 1 if effective_holding == 'ç°é‡‘' else 0
        }
        for w in WINDOWS:
             for f in ['ret', 'vol', 'slope', 'r2', 'mdd', 'sxr', 'sharp']:
                 cash_feat[f'{f}_{w}'] = 0.0
        daily_rows.append(cash_feat)
        
        # DataFrame & Rank
        df_day = pd.DataFrame(daily_rows)
        feature_cols = []
        for w in WINDOWS:
            feature_cols.extend([f'ret_{w}', f'vol_{w}', f'slope_{w}', f'r2_{w}', f'mdd_{w}', f'sxr_{w}', f'sharp_{w}'])
            
        for col in feature_cols:
            df_day[f'rank_{col}'] = df_day[col].rank(pct=True)
            
        # Context
        non_cash = df_day[df_day['name'] != 'ç°é‡‘']
        if not non_cash.empty:
            df_day['market_max_slope'] = non_cash['slope_23'].max()
            df_day['market_max_ret'] = non_cash['ret_23'].max()
        else:
            df_day['market_max_slope'] = 0
            df_day['market_max_ret'] = 0
            
        # Predict
        try:
            probs = predictor.predict_proba(df_day, model=model_name)
        except KeyError as e:
            st.error(f"âŒ ç‰¹å¾ç¼ºå¤±é”™è¯¯: {e}")
            st.write("å½“å‰ DataFrame åˆ—å:", df_day.columns.tolist())
            st.write("è¯·å°è¯•ç‚¹å‡»å·¦ä¾§ã€æ¸…é™¤ç¼“å­˜ã€‘æŒ‰é’®å¹¶é‡è¯•ã€‚")
            st.stop()
        if 1 in probs.columns:
            score_col = 1
        else:
            score_col = probs.columns[-1]
            
        df_day['score'] = probs[score_col]
        df_day = df_day.sort_values('score', ascending=False)
        
        # Decision
        top_pick = df_day.iloc[0]['name']
        top_score = df_day.iloc[0]['score']
        
        # Record
        # Calculate daily return for this day
        # Strategy Return:
        # If we held 'current_holding' coming INTO this day, we get its return.
        # But wait, decision is made at CLOSE? Or OPEN?
        # Usually backtest: Decision at Close T, Trade at Open T+1? Or Trade at Close T?
        # This strategy uses Close prices to decide.
        # Assuming we trade at Close T (Simulated).
        # So the return we get TODAY depends on what we held YESTERDAY.
        
        # Actually, let's simplify:
        # We hold 'prev_holding' from Yesterday Close to Today Close.
        # So Today's Strategy Return = Return of 'prev_holding'.
        
        daily_ret = 0.0
        holding_pct_chg = 0.0
        close_open_ratio = 0.0
        
        if current_holding and current_holding != 'ç°é‡‘':
             if current_holding in processed_data and d in processed_data[current_holding].index:
                 row_asset = processed_data[current_holding].loc[d]
                 # pct_change is (Close - PrevClose) / PrevClose
                 # We can use that directly from data if available, or calc it.
                 # row['ret_1'] is not exactly daily return if window is not 1.
                 # Let's use close / pre_close - 1
                 # But we pre-calculated ret_10, etc. Not ret_1.
                 # We have close. We need prev_close.
                 # Tushare data has 'pre_close'. If not, use shift.
                 
                 # Let's rely on data_dict original data for precision?
                 # processed_data is a copy.
                 
                 curr_close = row_asset['close']
                 # We need open for Close/Open ratio
                 # Tushare data has 'open'.
                 curr_open = row_asset.get('open', curr_close) # Fallback
                 
                 # Prev Close?
                 # We can't easily get prev row in this loop without index lookup.
                 # But 'ret_1' (if we had it) would be nice.
                 # Let's assume we can get it from 'ret_10' - no.
                 
                 # Quick fix: Calculate daily ret on the fly or pre-calc in prepare_all_features
                 # Let's assume pre_close is available or we can approximate.
                 # Actually, we can just fetch it from data_dict since we have the date.
                 # data_dict[current_holding]
                 
                 # Better: Pre-calculate daily_ret in prepare_all_features
                 daily_ret = row_asset.get('daily_ret', 0.0) 
                 holding_pct_chg = daily_ret
                 
                 if curr_open != 0:
                     close_open_ratio = curr_close / curr_open - 1
                 
        elif current_holding == 'ç°é‡‘':
            daily_ret = 0.0 # Cash return
            
        history.append({
            'date': d.date(),
            'holding': top_pick,
            'prev_holding': current_holding if current_holding else "ç©ºä»“(åˆå§‹)",
            'score': top_score,
            'action': 'Switch' if top_pick != current_holding else 'Hold',
            'daily_ret': daily_ret,
            'close_open_pct': close_open_ratio
        })
        
        # Update State
        current_holding = top_pick
        
    return pd.DataFrame(history), None

def update_data_process():
    """Run update_data.py as a subprocess"""
    try:
        # Pass current environment + secrets to subprocess
        env = os.environ.copy()
        
        # Try to get Token from Streamlit Secrets
        try:
            if 'TS_TOKEN' in st.secrets:
                env['TS_TOKEN'] = st.secrets['TS_TOKEN']
        except:
            pass # Ignore if secrets not available (local dev)
            
        result = subprocess.run([sys.executable, 'update_data.py'], capture_output=True, text=True, env=env)
        return result.returncode == 0, result.stdout + result.stderr
    except Exception as e:
        return False, str(e)

def calc_slope(y):
    # Safe check for NaN
    if len(y) < 2 or np.isnan(y).any(): return 0
    n = len(y)
    x = np.arange(n).reshape(-1, 1)
    
    # Avoid division by zero if y[0] is 0
    if y[0] == 0: return 0
    
    y_norm = y / y[0]
    model = LinearRegression().fit(x, y_norm)
    return model.coef_[0]

def calc_r2(y):
    if len(y) < 2 or np.isnan(y).any(): return 0
    n = len(y)
    x = np.arange(n).reshape(-1, 1)
    
    if y[0] == 0: return 0
    
    y_norm = y / y[0]
    model = LinearRegression().fit(x, y_norm)
    return model.score(x, y_norm)

def prepare_daily_features(data_dict, current_holding, target_date=None):
    all_dates = []
    for df in data_dict.values():
        all_dates.extend(df['date'].tolist())
    
    if not all_dates:
        return None, None, "No Data"
    
    # Filter dates
    unique_dates = sorted(list(set(all_dates)))
    
    if target_date is None:
        latest_date = unique_dates[-1]
    else:
        # Find closest date <= target_date
        target_ts = pd.Timestamp(target_date)
        valid_dates = [d for d in unique_dates if d <= target_ts]
        if not valid_dates:
            return None, None, "No data available before selected date"
        latest_date = valid_dates[-1]
    
    daily_snapshot = []
    
    # 1. Real Assets
    for name, df in data_dict.items():
        sub = df.set_index('date').sort_index()
        
        if latest_date in sub.index:
            idx = sub.index.get_loc(latest_date)
            if idx < 30: continue 
            
            # Logic for is_held
            is_held_val = 1 if current_holding == name else 0
            
            sample = {
                'name': name,
                'is_held': is_held_val
            }
            
            close_vals = sub['close'].values
            
            for w in WINDOWS:
                window_data = close_vals[idx-w+1 : idx+1]
                sample[f'ret_{w}'] = (window_data[-1] / window_data[0]) - 1
                pct_window = sub['close'].pct_change().values[idx-w+1 : idx+1]
                sample[f'vol_{w}'] = np.std(pct_window) * np.sqrt(252)
                sample[f'slope_{w}'] = calc_slope(window_data)
                sample[f'r2_{w}'] = calc_r2(window_data)
                sample[f'mdd_{w}'] = calc_max_drawdown(window_data)
                sample[f'sxr_{w}'] = sample[f'slope_{w}'] * sample[f'r2_{w}']
                sample[f'sharp_{w}'] = sample[f'slope_{w}'] / (sample[f'vol_{w}'] + 0.01)
                
            daily_snapshot.append(sample)
            
    # 2. Cash Asset
    cash_is_held = 1 if current_holding == 'ç°é‡‘' else 0
    cash_sample = {
        'name': 'ç°é‡‘',
        'is_held': cash_is_held
    }
    for w in WINDOWS:
        cash_sample[f'ret_{w}'] = 0.0
        cash_sample[f'vol_{w}'] = 0.0
        cash_sample[f'slope_{w}'] = 0.0
        cash_sample[f'r2_{w}'] = 0.0
        cash_sample[f'mdd_{w}'] = 0.0
        cash_sample[f'sxr_{w}'] = 0.0
        cash_sample[f'sharp_{w}'] = 0.0
    daily_snapshot.append(cash_sample)
    
    # 3. Ranking Features
    df_day = pd.DataFrame(daily_snapshot)
    
    feature_cols = []
    for w in WINDOWS:
        feature_cols.extend([f'ret_{w}', f'vol_{w}', f'slope_{w}', f'r2_{w}', f'mdd_{w}', f'sxr_{w}', f'sharp_{w}'])
        
    for col in feature_cols:
        df_day[f'rank_{col}'] = df_day[col].rank(pct=True)
        
    # Market Context
    non_cash = df_day[df_day['name'] != 'ç°é‡‘']
    if not non_cash.empty:
        df_day['market_max_slope'] = non_cash['slope_23'].max()
        df_day['market_max_ret'] = non_cash['ret_23'].max()
    else:
        df_day['market_max_slope'] = 0
        df_day['market_max_ret'] = 0
    
    return df_day, latest_date, None

def get_model_predictions(predictor, df_features, selected_models):
    """
    Returns a dict of {model_name: df_with_score}
    """
    results = {}
    
    for model_name in selected_models:
        # Clone df to avoid overwriting
        df_model = df_features.copy()
        
        # Predict
        try:
            probs = predictor.predict_proba(df_model, model=model_name)
            if 1 in probs.columns:
                score_col = 1
            else:
                score_col = probs.columns[-1]
            
            df_model['score'] = probs[score_col]
            df_model = df_model.sort_values('score', ascending=False)
            results[model_name] = df_model
        except Exception as e:
            st.warning(f"Model {model_name} prediction failed: {e}")
            
    return results

# --- UI ---

st.title("ğŸŒ¸ èŠ±å§‘å¨˜ 2.0 AI æŠ•é¡¾åŠ©æ‰‹")
st.markdown("åŸºäº **AutoGluon** å¤šæ¨¡å‹é›†æˆä¸å¯¹æ¯”")

# Display current active model version
st.info(f"ğŸ§¬ å½“å‰æ´»è·ƒæ¨¡å‹ç‰ˆæœ¬: **{st.session_state.selected_version}**")
if not os.path.exists(os.path.join(MODEL_PATH, 'predictor.pkl')):
    st.error(f"Debug: è·¯å¾„æœªæ‰¾åˆ° - {os.path.join(MODEL_PATH, 'predictor.pkl')}")

# Load Resources First to get model names
with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹ä¸æ•°æ®..."):
    data_dict = load_market_data()
    try:
        # å…ˆæ£€æŸ¥æ ¸å¿ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå†è¿›å…¥ç¼“å­˜åŠ è½½ï¼Œé¿å…ç¼“å­˜äº†é”™è¯¯çš„ç»“æœ
        predictor_file = os.path.join(MODEL_PATH, 'predictor.pkl')
        if not os.path.exists(predictor_file):
            predictor = None
        else:
            predictor = load_model(MODEL_PATH)
            
        if predictor is None:
            st.warning(f"âš ï¸ æ¨¡å‹ç‰ˆæœ¬ **{st.session_state.selected_version}** å°šæœªè®­ç»ƒå®Œæˆï¼Œéƒ¨åˆ†åŠŸèƒ½æš‚ä¸å¯ç”¨ã€‚è¯·åœ¨ä¾§è¾¹æ åˆ‡æ¢ç‰ˆæœ¬æˆ–ç­‰å¾…è®­ç»ƒç»“æŸã€‚")
            model_loaded = False
            available_models = []
        else:
            model_loaded = True
            available_models = predictor.model_names()
            # Default models: WeightedEnsemble_L2 (Best), CatBoost, XGBoost
            default_models = []
            best_model = predictor.model_best
            if best_model in available_models: default_models.append(best_model)
            if 'CatBoost' in available_models: default_models.append('CatBoost')
            
            # Fallback if specific names differ
            if not default_models: default_models = available_models[:1]
        
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        model_loaded = False
        available_models = []

# Sidebar
st.sidebar.header("âš™ï¸ å‚æ•°è®¾ç½®")

# Model Version Selection
st.sidebar.subheader("ğŸ¤– æ¨¡å‹ç‰ˆæœ¬")
selected_v = st.sidebar.selectbox(
    "é€‰æ‹© AI æ¨¡å‹ç‰ˆæœ¬",
    options=list(MODEL_VERSIONS.keys()),
    index=list(MODEL_VERSIONS.keys()).index(st.session_state.selected_version)
)
if selected_v != st.session_state.selected_version:
    st.session_state.selected_version = selected_v
    st.rerun()

# Navigation
st.sidebar.subheader("ğŸ“ å¯¼èˆª")
if st.sidebar.button("ğŸ“Š æŠ•é¡¾æ§åˆ¶å°"):
    navigate_to("dashboard")
if st.sidebar.button("ğŸ“š å…³äºæ¨¡å‹åŸç†"):
    navigate_to("about")
if st.sidebar.button("ğŸ¯ é•œåƒç­–ç•¥ä¸­å¿ƒ"):
    navigate_to("mirror")

# Data Update
if st.sidebar.button("ğŸ”„ æ›´æ–°å¸‚åœºæ•°æ® (Tushare)"):
    with st.spinner("æ­£åœ¨ä» Tushare æ‹‰å–æœ€æ–°æ—¥çº¿æ•°æ®..."):
        success, logs = update_data_process()
        if success:
            st.sidebar.success("æ•°æ®æ›´æ–°æˆåŠŸï¼")
            load_market_data.clear() # Clear cache to reload
            st.rerun()
        else:
            st.sidebar.error("æ•°æ®æ›´æ–°å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
            st.sidebar.text_area("é”™è¯¯æ—¥å¿—", logs)

if st.sidebar.button("ğŸ§¹ æ¸…é™¤ç³»ç»Ÿç¼“å­˜"):
    st.cache_data.clear()
    st.cache_resource.clear()
    st.sidebar.success("ç¼“å­˜å·²æ¸…é™¤ï¼")
    st.rerun()

# --- Main Routing ---

# Default to dashboard if page is not set
if 'page' not in st.session_state:
    st.session_state.page = "dashboard"

if st.session_state.page == "about":
    st.title("ğŸ“š èŠ±å§‘å¨˜ 2.0 é¡¹ç›®ç™½çš®ä¹¦")
    
    st.markdown("""
    æ¬¢è¿ä½¿ç”¨ **èŠ±å§‘å¨˜ 2.0 AI æŠ•é¡¾ç³»ç»Ÿ**ã€‚æœ¬é¡¹ç›®æ—¨åœ¨é€šè¿‡æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå¤åˆ»å¹¶è¶…è¶Šä¼˜ç§€çš„é‡åŒ–äº¤æ˜“ç­–ç•¥ã€‚
    
    ---
    
    ### ğŸ—ºï¸ é¡¹ç›®å…¨æ™¯å›¾
    
    æœ¬ç³»ç»Ÿç”±ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼Œå½¢æˆäº†ä¸€ä¸ªå®Œæ•´çš„é—­ç¯ï¼š
    
    1.  **æ•°æ®ä¸­å¿ƒ (Data Hub)**: è´Ÿè´£ä» Tushare ç­‰æ•°æ®æºæ‹‰å–å…¨çƒæ ¸å¿ƒèµ„äº§çš„æ—¥çº¿è¡Œæƒ…ã€‚
    2.  **AI å¤§è„‘ (Brain)**: åŸºäº AutoGluon çš„é›†æˆå­¦ä¹ æ¨¡å‹ï¼Œæ¯æ—¥è®¡ç®— 96 ä¸ªé‡åŒ–ç‰¹å¾ï¼Œè¾“å‡ºä¹°å–ä¿¡å·ã€‚
    3.  **å†³ç­–ç»ˆç«¯ (Dashboard)**: å³æ‚¨å½“å‰çœ‹åˆ°çš„ç•Œé¢ï¼Œæä¾›å•æ—¥å†³ç­–å»ºè®®å’Œå†å²å›æµ‹éªŒè¯ã€‚
    
    ---
    
    ### ğŸŒŠ æ•°æ®æµå‘ (Data Flow)
    
    1.  **åŸå§‹æ•°æ®**: `Open, High, Low, Close, Volume` (æ¯æ—¥æ›´æ–°)
        â¬‡ï¸
    2.  **ç‰¹å¾å·¥ç¨‹**: è®¡ç®— `Ret`, `Slope`, `R2`, `MaxDD`, `Vol` (8ä¸ªæ—¶é—´çª—å£)
        â¬‡ï¸
    3.  **æ¨¡å‹é¢„æµ‹**: è¾“å…¥ç‰¹å¾çŸ©é˜µ -> å¤šä¸ªæ¨¡å‹å¹¶è¡Œæ‰“åˆ† -> åŠ æƒé›†æˆ
        â¬‡ï¸
    4.  **æœ€ç»ˆå†³ç­–**: è¾“å‡º Score (0~1) -> ç»“åˆå½“å‰æŒä»“ç”Ÿæˆæ“ä½œæŒ‡ä»¤ (ä¹°å…¥/å–å‡º/è°ƒä»“)
    
    ---
    
    ### ğŸ§  æ ¸å¿ƒæ¨¡å‹åŸç†
    
    #### 1. è¡Œä¸ºå…‹éš† (Behavioral Cloning)
    æˆ‘ä»¬ä¸ç›´æ¥é¢„æµ‹è‚¡ä»·æ¶¨è·Œï¼Œè€Œæ˜¯**æ¨¡ä»¿ä¸“å®¶ç­–ç•¥**ã€‚
    *   **ä¸“å®¶**: åŸå§‹çš„â€œèŠ±å§‘å¨˜è§„åˆ™Eâ€ç­–ç•¥ï¼ˆåŸºäºåŠ¨é‡çš„è¶‹åŠ¿è·Ÿè¸ªï¼‰ã€‚
    *   **å­¦ç”Ÿ**: AI æ¨¡å‹ã€‚å®ƒè§‚å¯Ÿä¸“å®¶åœ¨å†å²ä¸Šçš„æ¯ä¸€æ¬¡æ“ä½œï¼Œå­¦ä¹ å…¶å†³ç­–é€»è¾‘ã€‚
    
    #### 2. ç‰¹å¾ä½“ç³» (96ç»´)
    æ¨¡å‹è§‚å¯Ÿä¸–ç•Œçš„â€œçœ¼ç›â€ç”±ä»¥ä¸‹æŒ‡æ ‡æ„æˆï¼š
    
    | ç»´åº¦ | æ ¸å¿ƒæŒ‡æ ‡ | ä½œç”¨ |
    | :--- | :--- | :--- |
    | **åŠ¨é‡** | `ret_{w}` | æ•æ‰æ¶¨è·Œå¹…åº¦ |
    | **è¶‹åŠ¿** | `slope_{w}` | æ•æ‰ä¸Šæ¶¨é€Ÿåº¦ |
    | **ç¨³å¥æ€§** | `r2_{w}`, `sxr_{w}` | å‰”é™¤è™šå‡çªç ´ |
    | **é£é™©** | **`mdd_{w}`** | **æ ¸å¿ƒé¿é™©æŒ‡æ ‡** (æœ€å¤§å›æ’¤) |
    | **æ³¢åŠ¨** | `vol_{w}` | è¡¡é‡ä¸ç¡®å®šæ€§ |
    | **æ’å** | `rank_{feature}` | å¯»æ‰¾ç›¸å¯¹æœ€å¼ºæ ‡çš„ |
    
    *æ³¨ï¼š`w` ä»£è¡¨æ—¶é—´çª—å£ï¼Œè¦†ç›– `[3, 5, 10, 20, 23, 30, 60, 120]` æ—¥ã€‚*
    
    #### 3. æ¨¡å‹çŸ©é˜µ
    *   **WeightedEnsemble_L2**: ğŸ‘‘ ç»¼åˆèƒ½åŠ›æœ€å¼ºï¼Œå®ƒä¼šè‡ªåŠ¨æƒè¡¡å„ä¸ªå­æ¨¡å‹çš„æ„è§ã€‚
    *   **CatBoost**: ååº”æ•æ·ï¼Œæ“…é•¿å¤„ç†çªå‘ç‰¹å¾ã€‚
    *   **LinearRegression**: ä¼ ç»Ÿçš„çº¿æ€§åŸºå‡†ï¼Œé€»è¾‘é€æ˜ (`0.5*æ”¶ç›Š + 0.5*è¶‹åŠ¿`)ã€‚
    
    ---
    
    ### ğŸ“– ä½¿ç”¨æŒ‡å—
    
    #### åœºæ™¯ A: æ¯å¤©æ—©ä¸Šæ€ä¹ˆåšï¼Ÿ
    1.  ç‚¹å‡»å·¦ä¾§ **â€œğŸ”„ æ›´æ–°å¸‚åœºæ•°æ®â€**ï¼Œç¡®ä¿æ•°æ®æœ€æ–°ã€‚
    2.  è¿›å…¥ **â€œğŸ“Š æŠ•é¡¾æ§åˆ¶å°â€** -> **â€œå•æ—¥å†³ç­–â€**ã€‚
    3.  é€‰æ‹©æ‚¨çš„ **â€œå½“å‰æŒä»“çŠ¶æ€â€** (ä¾‹å¦‚ï¼šç©ºä»“ï¼Œæˆ–æŒæœ‰çº³æŒ‡)ã€‚
    4.  ç‚¹å‡» **â€œğŸš€ ç”Ÿæˆå¤šæ¨¡å‹å†³ç­–â€**ã€‚
    5.  **æ‰§è¡ŒæŒ‡ä»¤**:
        *   âœ… **å»ºè®®ä¹°å…¥**: æ»¡ä»“ä¹°å…¥æ¨èæ ‡çš„ã€‚
        *   ğŸ”„ **å»ºè®®è°ƒä»“**: å–å‡ºå½“å‰æŒä»“ï¼Œä¹°å…¥æ–°æ¨èæ ‡çš„ã€‚
        *   â›”ï¸ **å»ºè®®è§‚æœ›/æ¸…ä»“**: å–å‡ºæ‰€æœ‰æŒä»“ï¼ŒæŒæœ‰ç°é‡‘ã€‚
    
    #### åœºæ™¯ B: éªŒè¯ç­–ç•¥é è°±å—ï¼Ÿ
    1.  è¿›å…¥ **â€œğŸ“Š æŠ•é¡¾æ§åˆ¶å°â€** -> **â€œåŒºé—´å›æµ‹â€**ã€‚
    2.  é€‰æ‹©ä¸€æ®µå†å²æ—¶æœŸ (å¦‚ 2020-2023)ã€‚
    3.  å‹¾é€‰ **â€œğŸ¦… ç‹©çŒæ¨¡å¼â€** (æ›´ä¸¥æ ¼çš„æµ‹è¯•æ ‡å‡†)ã€‚
    4.  ç‚¹å‡»å›æµ‹ï¼Œè§‚å¯Ÿ **â€œæœ€å¤§å›æ’¤â€** å’Œ **â€œå¹´åŒ–æ”¶ç›Šâ€**ã€‚
    
    ---
    
    ### âš ï¸ é£é™©æç¤º
    *   **å†å²ä¸ä»£è¡¨æœªæ¥**: AI æ˜¯åŸºäºå†å²è§„å¾‹è®­ç»ƒçš„ï¼Œé‡åˆ°å‰æ‰€æœªè§çš„é»‘å¤©é¹…äº‹ä»¶å¯èƒ½ä¼šå¤±æ•ˆã€‚
    *   **æ•°æ®å»¶è¿Ÿ**: å†³ç­–å»ºè®®åŸºäºæ”¶ç›˜ä»·ï¼Œå®ç›˜æ“ä½œå¯èƒ½å­˜åœ¨æ»‘ç‚¹ã€‚
    *   **éæŠ•èµ„å»ºè®®**: æœ¬ç³»ç»Ÿä»…ä¾›è¾…åŠ©å†³ç­–ï¼Œç›ˆäºè‡ªè´Ÿã€‚
    """)
    st.info("ğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥åœ¨å·¦ä¾§å¯¼èˆªæ è¿”å›ã€æŠ•é¡¾æ§åˆ¶å°ã€‘è¿›è¡Œå®é™…æ“ä½œã€‚")

elif st.session_state.page == "mirror":
    st.title("ğŸ¯ é•œåƒç­–ç•¥ä¸­å¿ƒ")
    st.markdown("---")
    st.caption("ä»¥ä¸‹å†…å®¹åŒæ­¥è‡ªå¤–éƒ¨ä¼˜ç§€ç­–ç•¥é•œåƒï¼Œä»…ä¾›å¯¹æ¯”å‚è€ƒã€‚")
    
    # ä½¿ç”¨ iframe åµŒå…¥é•œåƒç½‘ç«™ï¼Œç”¨æˆ·åœ¨åœ°å€æ åªèƒ½çœ‹åˆ°å½“å‰ç½‘ç«™çš„ URL
    # è¿™å®ç°äº†â€œéšè—çœŸå®åœ°å€â€çš„éœ€æ±‚
    st.components.v1.iframe("https://168.unicornhunter.cn/", height=1000, scrolling=True)

elif st.session_state.page == "dashboard":
    # Mode Selection
    mode = st.sidebar.radio("é€‰æ‹©æ¨¡å¼", ["å•æ—¥å†³ç­–", "åŒºé—´å›æµ‹ (Backtest)"])
    
    if mode == "å•æ—¥å†³ç­–":
        current_holding_option = st.sidebar.selectbox(
            "å½“å‰æŒä»“çŠ¶æ€",
            ['ç©ºä»“ (ç°é‡‘)'] + VALID_ASSETS
        )

        # Model Selection
        if model_loaded:
            st.sidebar.subheader("ğŸ§  æ¨¡å‹é€‰æ‹©")
            selected_models = st.sidebar.multiselect(
                "é€‰æ‹©å¯¹æ¯”æ¨¡å‹",
                available_models,
                default=default_models
            )
            primary_model = st.sidebar.selectbox(
                "ä¸»å†³ç­–æ¨¡å‹",
                selected_models,
                index=0 if selected_models else 0
            )
            
            # Date Selection
            st.sidebar.subheader("ğŸ“… æ—¥æœŸå›æº¯")
            
            # Get max date from data
            all_d = []
            for df in data_dict.values(): all_d.extend(df['date'].tolist())
            max_d = max(all_d).date() if all_d else datetime.date.today()
            min_d = min(all_d).date() if all_d else max_d
            
            selected_date = st.sidebar.date_input(
                "é€‰æ‹©å†³ç­–æ—¥æœŸ",
                value=max_d,
                min_value=min_d,
                max_value=max_d
            )
        else:
            selected_models = []
            primary_model = None
            selected_date = None

        # Map UI selection to code
        if 'ç©ºä»“' in current_holding_option:
            current_holding = None # Fresh Entry
            holding_display = "ç°é‡‘/ç©ºä»“"
        else:
            current_holding = current_holding_option
            holding_display = current_holding_option

        if model_loaded and selected_models:
            if st.button("ğŸš€ ç”Ÿæˆå¤šæ¨¡å‹å†³ç­–", type="primary"):
                df_features, date, err = prepare_daily_features(data_dict, current_holding, target_date=selected_date)
                
                if df_features is not None:
                    st.markdown(f"### ğŸ“… å†³ç­–åŸºå‡†æ—¥: {date.date()}")
                    
                    # Run Predictions
                    results = get_model_predictions(predictor, df_features, selected_models)
                    
                    if not results:
                        st.error("æ²¡æœ‰æ¨¡å‹è¿”å›æœ‰æ•ˆç»“æœã€‚")
                        st.stop()

                    # --- Primary Decision ---
                    df_primary = results[primary_model]
                    top_cand = df_primary.iloc[0]
                    top_name = top_cand['name']
                    top_score = top_cand['score']
                    
                    # Logic
                    action_color = "green"
                    action_text = ""
                    reason_text = ""
                    
                    if current_holding is None or current_holding == 'ç°é‡‘':
                        if top_name == 'ç°é‡‘':
                            action_text = "â›”ï¸ å»ºè®®è§‚æœ› (ä¿æŒç©ºä»“)"
                            action_color = "gray"
                            reason_text = "å¸‚åœºé£é™©è¾ƒé«˜ï¼Œä¸»æ¨¡å‹è®¤ä¸ºæŒæœ‰ç°é‡‘æ˜¯æœ€ä¼˜è§£ã€‚"
                        else:
                            action_text = f"âœ… å»ºè®®ä¹°å…¥: {top_name}"
                            action_color = "green"
                            reason_text = f"ä¸»æ¨¡å‹ ({primary_model}) ç»¼åˆè¯„åˆ†æœ€é«˜ ({top_score:.4f})ã€‚"
                    else:
                        if top_name == current_holding:
                            action_text = f"ğŸ”’ å»ºè®®æŒä»“: {current_holding}"
                            action_color = "blue"
                            reason_text = f"å½“å‰æŒä»“è¡¨ç°ç¨³å¥ (å¾—åˆ† {top_score:.4f})ã€‚"
                        else:
                            if top_name == 'ç°é‡‘':
                                action_text = f"âš ï¸ å»ºè®®æ¸…ä»“ -> ç°é‡‘"
                                action_color = "red"
                                reason_text = f"æŒæœ‰æ ‡çš„è½¬å¼±ï¼Œå»ºè®®é¿é™©ã€‚"
                            else:
                                action_text = f"ğŸ”„ å»ºè®®è°ƒä»“: {current_holding} -> {top_name}"
                                action_color = "orange"
                                reason_text = f"å‘ç°æ›´ä¼˜æ ‡çš„ï¼Œå¾—åˆ†ä¼˜åŠ¿æ˜¾è‘— ({top_score:.4f})ã€‚"

                    st.info(f"**å½“å‰çŠ¶æ€**: {holding_display}")
                    
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.subheader(f"ğŸ“¢ æ ¸å¿ƒæŒ‡ä»¤ (åŸºäº {primary_model})")
                        if action_color == 'green': st.success(action_text)
                        elif action_color == 'red': st.error(action_text)
                        elif action_color == 'blue': st.info(action_text)
                        elif action_color == 'orange': st.warning(action_text)
                        else: st.write(action_text)
                        st.markdown(f"**ğŸ’¡ å†³ç­–ç†ç”±**: {reason_text}")
                        
                    with col2:
                        st.metric("ä¸»æ¨¡å‹ç¡®ä¿¡åº¦", f"{top_score:.2%}")

                    # --- Model Comparison Table ---
                    st.subheader("ğŸ¤ å¤šæ¨¡å‹å…±è¯†åˆ†æ")
                    st.caption(f"æ³¨ï¼šä»¥ä¸‹é¢„æµ‹å‡åŸºäºå½“å‰æ´»è·ƒç‰ˆæœ¬ï¼š{st.session_state.selected_version}")
                    
                    comp_data = []
                    for m_name, res_df in results.items():
                        top_row = res_df.iloc[0]
                        # Check consensus
                        action_type = "æŒä»“" if top_row['name'] == current_holding else ("ä¹°å…¥" if current_holding is None else "è°ƒä»“")
                        if top_row['name'] == 'ç°é‡‘' and current_holding is not None and current_holding != 'ç°é‡‘':
                            action_type = "æ¸…ä»“"
                        elif top_row['name'] == 'ç°é‡‘' and (current_holding is None or current_holding == 'ç°é‡‘'):
                            action_type = "è§‚æœ›"
                        
                        comp_data.append({
                            "æ¨¡å‹åç§°": m_name,
                            "é¦–é€‰æ ‡çš„": top_row['name'],
                            "ç¡®ä¿¡åº¦ (Score)": f"{top_row['score']:.2%}",
                            "å»ºè®®åŠ¨ä½œ": action_type,
                            "23æ—¥è¶‹åŠ¿": f"{top_row['rank_slope_23']:.2f}"
                        })
                    
                    st.dataframe(pd.DataFrame(comp_data), use_container_width=True)
                    
                    # --- Detailed Breakdown ---
                    st.subheader(f"ğŸ“Š èµ„äº§è¯„åˆ†è¯¦æƒ… ({primary_model})")
                    
                    # Chart
                    chart_df = df_primary.head(10).copy()
                    c = alt.Chart(chart_df).mark_bar().encode(
                        x=alt.X('score', title='è¯„åˆ†'),
                        y=alt.Y('name', sort='-x', title='èµ„äº§'),
                        color=alt.condition(
                            alt.datum.name == top_name,
                            alt.value('orange'),
                            alt.value('steelblue')
                        ),
                        tooltip=['name', 'score', 'ret_23']
                    ).properties(height=350)
                    st.altair_chart(c, use_container_width=True)
                    
                else:
                    st.error("æ— æ³•ç”Ÿæˆé¢„æµ‹ï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")
            else:
                st.info("è¯·ç‚¹å‡»æŒ‰é’®ç”Ÿæˆå†³ç­–")

    elif mode == "åŒºé—´å›æµ‹ (Backtest)":
        st.header("ğŸ“ˆ å†å²åŒºé—´å›æµ‹æ¨¡æ‹Ÿ")
        
        if model_loaded:
            # Backtest Settings
            col1, col2 = st.columns(2)
        
        all_d = []
        for df in data_dict.values(): all_d.extend(df['date'].tolist())
        if not all_d:
            st.error("æ— æ•°æ®")
            st.stop()
            
        max_d = max(all_d).date()
        min_d = min(all_d).date()
        
        # Enforce min date restriction
        limit_min_d = datetime.date(2017, 8, 1)
        if min_d < limit_min_d:
            min_d = limit_min_d
        
        with col1:
            start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=max_d - datetime.timedelta(days=365*2), min_value=min_d, max_value=max_d)
        with col2:
            end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=max_d, min_value=min_d, max_value=max_d)
            
        col3, col4 = st.columns(2)
        with col3:
            # Allow selecting across versions
            compare_versions = st.multiselect(
                "é€‰æ‹©æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå¯¹æ¯”",
                options=list(MODEL_VERSIONS.keys()),
                default=[st.session_state.selected_version]
            )
            bt_models = st.multiselect("é€‰æ‹©å­æ¨¡å‹ (å„ç‰ˆæœ¬é€šç”¨)", available_models, default=default_models)
        with col4:
            init_hold = st.selectbox("åˆå§‹æŒä»“", ["ç©ºä»“ (Neutral)"] + VALID_ASSETS)
            force_neutral = st.checkbox("ğŸ¦… ç‹©çŒæ¨¡å¼ (æ¯æ—¥å‡è®¾ç©ºä»“ï¼Œæ— è§†æŒä»“Buffer)", value=True, help="å‹¾é€‰åï¼Œæ¨¡å‹æ¯å¤©éƒ½ä¼šå‡è®¾å½“å‰æ˜¯ç©ºä»“çŠ¶æ€è¿›è¡Œè¯„åˆ†ã€‚")
            use_warmup = st.checkbox("ğŸ”¥ ä½¿ç”¨å†å²æ•°æ®é¢„çƒ­", value=False, help="é»˜è®¤å…³é—­ã€‚å›æµ‹ç¬¬ä¸€å¤©å°†ä¸ä½¿ç”¨å¼€å§‹æ—¥æœŸä¹‹å‰çš„ä»»ä½•æ•°æ®ã€‚")
            
        real_init = None if "ç©ºä»“" in init_hold else init_hold
        
        if st.button("â–¶ï¸ å¼€å§‹å›æµ‹", type="primary"):
            if start_date >= end_date:
                st.error("å¼€å§‹æ—¥æœŸå¿…é¡»æ—©äºç»“æŸæ—¥æœŸ")
            elif not bt_models or not compare_versions:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰ˆæœ¬å’Œä¸€ä¸ªæ¨¡å‹")
            else:
                results_df = []
                progress_text = st.empty()
                
                total_runs = len(compare_versions) * len(bt_models)
                run_idx = 0
                
                for v_name in compare_versions:
                    v_path = MODEL_VERSIONS[v_name]
                    with st.spinner(f"æ­£åœ¨åŠ è½½ {v_name}..."):
                        v_predictor = load_model(v_path)
                    
                    if v_predictor is None:
                        st.error(f"âŒ æ¨¡å‹ {v_name} å°šæœªè®­ç»ƒå®Œæˆæˆ–è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·ç­‰å¾…è®­ç»ƒç»“æŸã€‚")
                        continue
                    
                    for m_name in bt_models:
                        run_idx += 1
                        display_name = f"{v_name} - {m_name}" if len(compare_versions) > 1 else m_name
                        progress_text.text(f"æ­£åœ¨å›æµ‹: {display_name} ({run_idx}/{total_runs})...")
                        
                        df_hist, err = run_backtest_range(
                            v_predictor, data_dict, start_date, end_date, m_name,
                            initial_holding=real_init,
                            force_neutral=force_neutral,
                            use_warmup=use_warmup
                        )
                        
                        if df_hist is not None:
                            df_hist['Model'] = display_name
                            df_hist['cumulative_ret'] = (1 + df_hist['daily_ret']).cumprod()
                            results_df.append(df_hist)
                
                progress_text.empty()
                
                if results_df:
                    st.success("å›æµ‹å®Œæˆï¼")
                    
                    all_res = pd.concat(results_df)
                    
                    # --- Comparison Chart ---
                    st.subheader("ğŸ“ˆ å¤šæ¨¡å‹å‡€å€¼å¯¹æ¯”")
                    
                    chart_comp = alt.Chart(all_res).mark_line().encode(
                        x=alt.X('date:T', title='æ—¥æœŸ'),
                        y=alt.Y('cumulative_ret', title='ç´¯è®¡å‡€å€¼', scale=alt.Scale(zero=False)),
                        color='Model',
                        tooltip=['date', 'Model', 'cumulative_ret', 'holding']
                    ).interactive()
                    
                    st.altair_chart(chart_comp, use_container_width=True)
                    
                    # --- Metrics Table ---
                    # --- Metrics Table ---
                    st.subheader("ğŸ“Š ç»©æ•ˆæŒ‡æ ‡å¯¹æ¯”")
                    
                    metrics_data = []
                    # Get unique model names in order of results_df
                    actual_models_list = [df['Model'].iloc[0] for df in results_df]
                    
                    for m_name in actual_models_list:
                        sub = all_res[all_res['Model'] == m_name]
                        total_days = (sub['date'].max() - sub['date'].min()).days
                        if total_days < 1: total_days = 1
                        
                        total_ret = sub['cumulative_ret'].iloc[-1] - 1
                        cagr = (1 + total_ret) ** (365 / total_days) - 1
                        
                        # Daily returns for risk metrics
                        rets = sub['daily_ret']
                        vol = rets.std() * np.sqrt(252)
                        
                        # Downside deviation for Sortino
                        downside_rets = rets[rets < 0]
                        downside_std = downside_rets.std() * np.sqrt(252)
                        
                        rf = 0.02
                        sharpe = (cagr - rf) / vol if vol != 0 else 0
                        sortino = (cagr - rf) / downside_std if downside_std != 0 else 0
                        
                        # Drawdown
                        roll_max = sub['cumulative_ret'].cummax()
                        dd = (sub['cumulative_ret'] - roll_max) / roll_max
                        max_dd = dd.min()
                        
                        # Calmar
                        calmar = cagr / abs(max_dd) if max_dd != 0 else 0
                        
                        # Win Rate & Profit Factor
                        wins = rets[rets > 0]
                        losses = rets[rets < 0]
                        win_rate = len(wins) / len(rets[rets != 0]) if len(rets[rets != 0]) > 0 else 0
                        profit_factor = abs(wins.sum() / losses.sum()) if losses.sum() != 0 else float('inf')
                        
                        trade_count = len(sub[sub['action'] == 'Switch'])
                        
                        metrics_data.append({
                            "æ¨¡å‹": m_name,
                            "æ€»æ”¶ç›Š": f"{total_ret:.2%}",
                            "å¹´åŒ–æ”¶ç›Š": f"{cagr:.2%}",
                            "å¤æ™®æ¯”ç‡": f"{sharpe:.2f}",
                            "ç´¢æè¯ºæ¯”ç‡": f"{sortino:.2f}",
                            "å¡ç›æ¯”ç‡": f"{calmar:.2f}",
                            "æœ€å¤§å›æ’¤": f"{max_dd:.2%}",
                            "èƒœç‡(æ—¥)": f"{win_rate:.2%}",
                            "ç›ˆäºæ¯”": f"{profit_factor:.2f}",
                            "äº¤æ˜“æ¬¡æ•°": trade_count
                        })
                        
                    st.dataframe(pd.DataFrame(metrics_data), use_container_width=True)

                    # --- Metrics Explanation ---
                    with st.expander("ğŸ“š ç‚¹å‡»æŸ¥çœ‹é‡‘èç»©æ•ˆæŒ‡æ ‡è§£é‡Š"):
                        st.markdown("""
                        | æŒ‡æ ‡ | è§£é‡Š | é€šä¿—ç†è§£ |
                        | :--- | :--- | :--- |
                        | **æ€»æ”¶ç›Š** | å›æµ‹æœŸå†…çš„ç´¯è®¡å›æŠ¥ç‡ã€‚ | æœ€ç»ˆèµšäº†å¤šå°‘é’±ã€‚ |
                        | **å¹´åŒ–æ”¶ç›Š** | å°†æ€»æ”¶ç›Šè½¬åŒ–æˆæ¯å¹´çš„å¹³å‡æ”¶ç›Šã€‚ | ç›¸å½“äºå­˜é“¶è¡Œçš„â€œå¹´åˆ©ç‡â€ã€‚ |
                        | **å¤æ™®æ¯”ç‡** | æ¯æ‰¿æ‹…ä¸€å•ä½æ€»é£é™©ï¼Œæ‰€è·å¾—çš„è¶…é¢æ”¶ç›Šã€‚ | **è¶Šé«˜è¶Šå¥½**ã€‚åæ˜ äº†èµšé’±çš„â€œæ€§ä»·æ¯”â€ï¼Œ1.0ä»¥ä¸Šç®—ä¸é”™ã€‚ |
                        | **ç´¢æè¯ºæ¯”ç‡** | ä¸“é—¨è¡¡é‡æ‰¿æ‹…â€œä¸‹è·Œé£é™©â€è·å¾—çš„æ”¶ç›Šã€‚ | ç›¸æ¯”å¤æ™®ï¼Œå®ƒä¸æƒ©ç½šå‘ä¸Šçš„æ³¢åŠ¨ï¼Œæ›´çœ‹é‡æŠ—è·Œèƒ½åŠ›ã€‚ |
                        | **å¡ç›æ¯”ç‡** | å¹´åŒ–æ”¶ç›Šä¸æœ€å¤§å›æ’¤çš„æ¯”å€¼ã€‚ | è¡¡é‡â€œä¸ºäº†èµšé’±ï¼Œä½ èƒ½å¿å—å¤šå¤§çš„äºæŸâ€ï¼Œåæ˜ äº†æ”¶ç›Šé£é™©æ¯”ã€‚ |
                        | **æœ€å¤§å›æ’¤** | å‡€å€¼ä»æœ€é«˜ç‚¹å›è½åˆ°æœ€ä½ç‚¹çš„æœ€å¤§å¹…åº¦ã€‚ | å†å²ä¸Šâ€œæœ€æƒ¨â€çš„æ—¶å€™äºäº†å¤šå°‘ï¼Œè€ƒéªŒæŠ•èµ„è€…çš„å¿ƒè„æ‰¿å—åŠ›ã€‚ |
                        | **èƒœç‡(æ—¥)** | èµšé’±çš„å¤©æ•°å æ€»äº¤æ˜“å¤©æ•°çš„æ¯”ä¾‹ã€‚ | æ¯å¤©çå¼€çœ¼ï¼Œèµšåˆ°é’±çš„æ¦‚ç‡ã€‚ |
                        | **ç›ˆäºæ¯”** | ç›ˆåˆ©æ€»é¢ä¸äºæŸæ€»é¢çš„æ¯”å€¼ã€‚ | èµšçš„æ—¶å€™èµšå¤šå°‘ï¼Œäºçš„æ—¶å€™äºå¤šå°‘ã€‚ |
                        | **äº¤æ˜“æ¬¡æ•°** | å‘ç”Ÿè°ƒä»“ï¼ˆå–å‡ºæ—§æ ‡çš„ä¹°å…¥æ–°æ ‡çš„ï¼‰çš„æ¬¡æ•°ã€‚ | åæ˜ äº†ç­–ç•¥çš„æ¢æ‰‹é¢‘ç‡ï¼Œæ¬¡æ•°å¤ªå¤šå¯èƒ½äº§ç”Ÿè¾ƒé«˜çš„æ‰‹ç»­è´¹ã€‚ |
                        """)

                    # --- Individual Details (Tabs) ---
                    st.subheader("ğŸ“Š æ¨¡å‹è¯¦ç»†è®°å½•")
                    tabs = st.tabs(actual_models_list)
                    
                    for i, m_name in enumerate(actual_models_list):
                        with tabs[i]:
                            sub = all_res[all_res['Model'] == m_name].copy()
                            
                            # Max Drawdown for chart
                            roll_max = sub['cumulative_ret'].cummax()
                            sub['drawdown'] = (sub['cumulative_ret'] - roll_max) / roll_max
                            max_dd = sub['drawdown'].min()
                            
                            # Drawdown Chart
                            c_dd = alt.Chart(sub).mark_area(color='red', opacity=0.3).encode(
                                x='date:T',
                                y=alt.Y('drawdown', title='å›æ’¤', scale=alt.Scale(domain=[max_dd*1.1, 0])),
                                tooltip=['date', 'drawdown']
                            ).properties(height=150)
                            st.altair_chart(c_dd, use_container_width=True)
                            
                            # Table
                            st.dataframe(
                                sub[['date', 'holding', 'action', 'score', 'daily_ret', 'cumulative_ret']].style.format({
                                    'score': '{:.4f}',
                                    'daily_ret': '{:.2%}',
                                    'cumulative_ret': '{:.4f}'
                                }), 
                                use_container_width=True
                            )
                            
                            # Holding Pie
                            h_counts = sub['holding'].value_counts().reset_index()
                            h_counts.columns = ['Asset', 'Days']
                            c_pie = alt.Chart(h_counts).mark_arc().encode(
                                theta='Days', color='Asset', tooltip=['Asset', 'Days']
                            )
                            st.altair_chart(c_pie)
                else:
                    st.error("å›æµ‹å¤±è´¥")
    else:
        st.error("æ¨¡å‹æœªåŠ è½½")


st.markdown("---")
st.caption("æ³¨ï¼šä¸åŒæ¨¡å‹å¯¹é£é™©çš„æ•æ„Ÿåº¦ä¸åŒï¼ŒWeightedEnsemble é€šå¸¸æœ€ç¨³å¥ï¼ŒCatBoost å¯¹ç±»åˆ«ç‰¹å¾æ›´æ•æ„Ÿã€‚")
